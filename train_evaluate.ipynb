{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a745eb4-3145-4cfc-9bda-ea77c4b7f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44956e8-b2d5-4fb9-9678-23fdd4990dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Clone the repo.\n",
    "    !git clone \"https://github.com/ash0ts/keras-image-captioning-wandb.git\"\n",
    "\n",
    "    # Change the working directory to the repo root.\n",
    "    %cd keras-image-captioning-wandb\n",
    "\n",
    "    # Add the repo root to the Python path.\n",
    "    import sys, os\n",
    "    sys.path.append(os.getcwd())\n",
    "    \n",
    "    !pip install wandb\n",
    "    !pip install PIL\n",
    "    \n",
    "    import wandb\n",
    "    \n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929318cc-686b-4f02-88ad-53ff7848c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314b3dd-3717-4b29-a882-778990948403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.CNNEncoder import CNN_Encoder\n",
    "from model.RNNDecoder import RNN_Decoder\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from image_utils import load_image\n",
    "import numpy as np\n",
    "from config import WANDB_PROJECT, WANDB_ENTITY, max_length, vocabulary_size, attention_features_shape\n",
    "import wandb\n",
    "from utils import load_from_pickle\n",
    "from text_utils import generate_text_artifacts\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0ce9a-722a-4794-93a6-1e58616f87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "        \"EPOCHS\": 20,\n",
    "        \"BATCH_SIZE\": 64,\n",
    "        \"BUFFER_SIZE\": 1000,\n",
    "        \"embedding_dim\": 256,\n",
    "        \"units\": 512,\n",
    "        \"features_shape\": 2048,\n",
    "        \"attention_features_shape\": 64\n",
    "}\n",
    "\n",
    "run = wandb.init(project=WANDB_PROJECT,\n",
    "                 entity=WANDB_ENTITY, name=\"train-coco2014-attention-model\", job_type=\"train\", config=default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d74d0-b79a-458b-b3c5-ef768acd417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = run.config.get(\"EPOCHS\")\n",
    "BATCH_SIZE = run.config.get(\"BATCH_SIZE\")\n",
    "BUFFER_SIZE = run.config.get(\"BUFFER_SIZE\")\n",
    "embedding_dim = run.config.get(\"embedding_dim\")\n",
    "units = run.config.get(\"units\")\n",
    "features_shape = run.config.get(\"features_shape\")\n",
    "attention_features_shape = run.config.get(\"attention_features_shape\")\n",
    "\n",
    "split_art = run.use_artifact(\"split:latest\")\n",
    "img_name_train_path = split_art.get_path(\"img_name_train.pkl\").download()\n",
    "cap_train_path = split_art.get_path(\"cap_train.pkl\").download()\n",
    "\n",
    "image_feat_art = run.use_artifact(\"inception_v3:latest\")\n",
    "image_feat_path = image_feat_art.download()\n",
    "img_name_train = [os.path.join(image_feat_path, path)\n",
    "                  for path in load_from_pickle(img_name_train_path)]\n",
    "cap_train = load_from_pickle(cap_train_path)\n",
    "\n",
    "# TODO: Very jank. We regenerate the tokenizer twice from these same data from a previous step which feels wrong. Hope this doesnt change.\n",
    "img_cap_table = run.use_artifact(\n",
    "    \"image_caption_table:latest\").get(\"img_cap_table\")\n",
    "train_captions = img_cap_table.get_column(\"caption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed998052-9398-41ae-80cb-e17f4f836e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n",
    "# cap vecotr contains each sentence as max_length where the word position index is the vocab index\n",
    "tokenizer, _, word_to_index, index_to_word = generate_text_artifacts(\n",
    "    caption_dataset, max_length=max_length, vocabulary_size=vocabulary_size, return_vector=False, return_mapping=True)\n",
    "\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# TODO: Load the numpy files from inception artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352efbad-cecd-4c75-810e-69931c0ce48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a614a5-42af-4ac9-949d-5a341f482e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "    map_func, [item1, item2], [tf.float32, tf.int64]),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d18b7d-ff84-4618-ae9d-bd9c5dde3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1153e-a0b5-4c5c-a804-cd5bb65e0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f13cf5-9090-4edc-a77b-abb07fb1a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76000d-acd3-4bad-9f73-a5af79717789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "\n",
    "@ tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims(\n",
    "        [word_to_index('<start>')] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5619ac-7e85-4aa0-bcab-83159e3342bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_gradients(model):\n",
    "    gradients = {}\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Dense):\n",
    "            gradients[layer.name] = layer.get_weights()[0]\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cdab4e-faa0-4e0d-9b88-66c544d729c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592683b-1676-4f4c-b15b-8901b8d2d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        print(batch)\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        run.log({\"batch_loss\": batch_loss})\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(\n",
    "                f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    run.log({\"epoch_loss\": total_loss/num_steps})\n",
    "    run.log({\"encoder_epoch_gradient\": grab_gradients(encoder)})\n",
    "    run.log({\"decoder_epoch_gradient\": grab_gradients(decoder)})\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c030ae5-978d-4ce9-810d-3c61dd320c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save(\"encoder\")\n",
    "decoder.save(\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f41c4f-3abb-4b65-8536-ddca21a6f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = wandb.Artifact(name=\"encoder\", type=\"model\")\n",
    "decoder_model = wandb.Artifact(name=\"decoder\", type=\"model\")\n",
    "checkpoints_art = wandb.Artifact(name=\"checkpoints\", type=\"training\")\n",
    "\n",
    "encoder_model.add_dir(\"encoder\")\n",
    "decoder_model.add_dir(\"decoder\")\n",
    "checkpoints_art.add_dir(\"checkpoints\")\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c30566-3a4c-4c07-aca2-ff8e9cbdd041",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_artifact(encoder_model)\n",
    "run.log_artifact(decoder_model)\n",
    "run.log_artifact(checkpoints_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75dddf-5539-4c12-8c36-d90212a9e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ccd112-3d05-450b-8e1a-7c92f1c0b8d6",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a10879-8bc1-430e-ab57-acfb9c6dc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d7520-66e1-4efe-b87a-a94d9f887203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model.CNNEncoder import CNN_Encoder\n",
    "from model.RNNDecoder import RNN_Decoder\n",
    "from text_utils import generate_text_artifacts\n",
    "from image_utils import load_image\n",
    "from utils import load_from_pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fef4e-b13e-4439-b847-ecc32ac52b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import WANDB_PROJECT, WANDB_ENTITY, max_length, vocabulary_size, attention_features_shape\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e752a9-336d-4359-b466-aa4d020825db",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=WANDB_PROJECT,\n",
    "                 entity=WANDB_ENTITY, name=\"evaluate-coco2014-attention-model\", job_type=\"evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25957a08-9bc8-4880-a873-2eb3cf2b7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cap_table = run.use_artifact(\n",
    "    \"image_caption_table:latest\").get(\"img_cap_table\")\n",
    "train_captions = img_cap_table.get_column(\"caption\")\n",
    "# TODO: Very jank. We regenerate the tokenizer twice from these same data from a previous step which feels wrong. Hope this doesnt change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f9784-fa07-43be-bfcc-e4ba25db4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n",
    "# cap vecotr contains each sentence as max_length where the word position index is the vocab index\n",
    "tokenizer, _, word_to_index, index_to_word = generate_text_artifacts(\n",
    "    caption_dataset, max_length=max_length, vocabulary_size=vocabulary_size, return_vector=False, return_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bad14-1119-4d65-bef2-79197e8e7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_extract_model_path = run.use_artifact(\n",
    "    \"feature_extractor:latest\").download()\n",
    "image_features_extract_model = tf.keras.models.load_model(image_features_extract_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e6041d-6265-4b9f-8dfc-bd892f5eb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_path = run.use_artifact(\"encoder:latest\").download()\n",
    "# encoder = tf.saved_model.load(encoder_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18973d69-3070-45d8-a7b8-2d1afd9e5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model_path = run.use_artifact(\"decoder:latest\").download()\n",
    "# decoder = tf.saved_model.load(decoder_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4aae8-27ec-4128-8567-66b393b94d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n",
    "        result.append(predicted_word)\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8749b-b65b-47d5-ba01-64cabd3cf758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    plt.clf()\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(int(np.ceil(len_result/2)), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948ed21-b198-4872-97b7-1445850bb1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_art = run.use_artifact(\"images:latest\")\n",
    "images_path = images_art.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094b0c6-590c-4718-afac-6b032e2fea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_art = run.use_artifact(\"split:latest\")\n",
    "img_name_val_path = split_art.get_path(\"img_name_val.pkl\").download()\n",
    "cap_val_path = split_art.get_path(\"cap_val.pkl\").download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4e55f-b52b-4df9-87c2-74b8a831d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_val = [os.path.join(images_path, path)\n",
    "                  for path in load_from_pickle(img_name_val_path)]\n",
    "cap_val = load_from_pickle(cap_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17e222-b6bd-4418-8109-fccae51176bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda176f9-03de-4e2d-af3a-7a49364729b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = os.path.join(\".\", \"attention_plots\")\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f991c-59d6-451e-a78b-13906a5f11b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_table = wandb.Table(columns = [\"name\", \"image\", \"real_caption\", \"predicted_caption\", \"attention_plot\"])\n",
    "# captions on the validation set\n",
    "for rid, image in tqdm(enumerate(img_name_val)):\n",
    "    print(rid)\n",
    "    real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy())\n",
    "                            for i in cap_val[rid] if i not in [0]])\n",
    "    result, attention_plot = evaluate(image)\n",
    "\n",
    "    print('Real Caption:', real_caption)\n",
    "    predicted_caption = ' '.join(result)\n",
    "    print('Prediction Caption:', predicted_caption)\n",
    "    plt = plot_attention(image, result, attention_plot)\n",
    "    plt_path = os.path.join(plot_dir, f'attention_{os.path.basename(image)}.png')\n",
    "    plt.savefig(plt_path)\n",
    "    eval_table.add_data(os.path.basename(image), wandb.Image(image), real_caption, predicted_caption, wandb.Image(plt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a4c53-0ca0-45a8-aedb-677e769dfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_table_art = wandb.Artifact(name=\"eval_table\", type=\"eval\")\n",
    "eval_table_art.add(eval_table, \"eval_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09657da-5e6c-49ab-82f0-83f92a453eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log({\"eval_table\": eval_table})\n",
    "run.log_artifact(eval_table_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15507ed5-f16f-40b2-ab5c-4e7f9cdedc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
